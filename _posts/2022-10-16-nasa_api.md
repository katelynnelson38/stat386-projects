---
layout: post
title:  "Using APIs: Access NASA data (IN PROGRESS)"
date:   2022-10-16
author: Katelyn Nelson
description: Learn how to use an API in Python to access NASA's global landslide data
image: /assets/images/1C1701ED-D459-4F9D-960D-6C10645CB3AE.JPEG
---

Every data analysis begins with 'data' (see what I did there? ðŸ˜‰). So where can you get data without putting the effort into collecting your own? An API (application programming interface) is a powerful tool that acts as a bridge between your device and a database so that you can access existing data. To use an API you need a unique 'key' that acts as a personal password to authenticate your access to the data. There are a lot of free APIs online that you can request keys for, but sometimes they require a paid subscription.

Even though the you can access data, make sure that you are using according to the source's conditions. I will be using NASA's Open Data Portal because has a ton of pictures, data, and other resources that are available for free as long as you agree to their Terms and Conditions and cite the data.

**Note:** I will not include all of my code in this post, but if you have difficulty following alon,, the Jupyter Notebook file that I used to complete the steps below can be found in my github repository [nasa_api](https://github.com/katelynnelson38/nasa_api).

# Set up

Before we can start, you will need to [create an account](https://data.nasa.gov/login) to NASA's Open Data Portal and [request an API key](https://api.nasa.gov/index.html#signUp). I recommend copying your API key and pasting it into text file to use for later use. In this post I will be using NASA's [Global Landslide Catalog Export](https://data.nasa.gov/Earth-Science/Global-Landslide-Catalog-Export/dd9e-wu2v) data.

I used the beautiful soup and requests packages in Python to use the NASA API, so make sure to import those. You'll need additional packages for the data cleaning, but I will not talk about those in depth.

```
from bs4 import BeautifulSoup
import requests
```

# Get the data

For privacy and legal purposes, I will not share my API key, but if you requested one it should work the same. I pasted mine into a file named `nasa_apikey.txt`. Substitute the name of your file, or the API key.

```
with open('nasa_apikey.txt', 'r') as file:
    nasa_key = file.read()

payload = {}

headers= {
  "apikey": nasa_key
}
```

Now you can use your key (stored as a header) to request data from the NASA database. Most often you specify which data you want by the url you use, which can usually be found in the online API documentation.

```
nasa_url = 'https://data.nasa.gov/resource/dd9e-wu2v.json'
r = requests.get(nasa_url, headers=headers, data = payload)
```

Use `.json()` to read the data and create a pandas dataframe.

```
landslide = r.json()
landslide_df = pd.DataFrame(landslide)
```

# Clean the data

Now that we have the data, we can use the `shape` attribute to see that it includes 1000 observations with 30 variables. Let's narrow it down to 9 variables that we might want to use for an analysis.

```
landslide_df = landslide_df[['event_date', 'landslide_category', 'landslide_trigger', 'landslide_size', 'fatality_count', 'injury_count', 'country_name', 'longitude', 'latitude']]
```

Use the `info()` function to understand the data better.
One powerful tool in exploring new data is the `info()` function. When you use the function on a dataframe (`df.info()`) it will return how many columns the dataframe has, how many non-null values each column has, and the variable type for each.

```
landslide_df.info()
```

![first info](https://raw.githubusercontent.com/katelynnelson38/stat386-projects/main/assets/images/apipost/first.info.PNG)

This data is has quite a few null values and all the variables are type object. We will have to fix this before the data is ready for an analysis. 

Start by doing the following...

- The null values in `fatality_count` and `injury_count` correspond with no fatalities or injuries
- When landslide_size, `landslide_trigger`, and `country_name` are null, we will fill it with `unknown`
- Columns `fatality_count`, `injury_count`, `latitude`, and `longitude` need to be type float
- Extract a simpler version of `event_date` (year-month-day) and convert it to type datetime

Does the formatting below look better?

- The null values in fatality_count and injury_count correspond with no fatalities or injuries
- When landslide_size, landslide_trigger, and country_name are null, we will fill it with 'unknown'
- Columns fatality_count, injury_count, latitude, and longitude need to be type float
- Extract a simpler version of event_date (year-month-day) and convert it to type datetime

(Remember, if you need help look at how I did it in my [nasa_api](https://github.com/katelynnelson38/nasa_api) repository.)

Using `info()` again after cleaning, we can see that our data does not have any null values and the columns are the correct variable type (hooray ðŸŽ‰).

![second info](https://raw.githubusercontent.com/katelynnelson38/stat386-projects/main/assets/images/apipost/second.info.PNG)

Your cleaned dataframe should resemble this:

![landslide df](https://raw.githubusercontent.com/katelynnelson38/stat386-projects/main/assets/images/apipost/cleaned_df.PNG)

Now the data is ready to be used for analysis!

# Export the data

Once you have your data, you may want to export it to share with others or for later use.

```
landslide_df.to_csv("landslide", index = False)
```

**Note:** Once you export the data to a csv, the variable types will be lost. If you read it in, all variables will be type object.

# You did it!

What now? Well, you can use the data to learn about common triggers of landslides, the effects of landslides, the frequency of landslides in a certain country, the most common time of year for landslides, etc. There are so many possibilties, so stay tuned ðŸ‘€ for my next post where I will do an EDA (exploratory data analysis) with the landslide dataframe we just created!

As you can see, there is so much data available for your exploration, you just need to know how to access it! APIs are powerful tools that allow you to access lots of cool data. Web scraping can also produce intersting data. I prefer using API's because they data usually turns out a little tidier, but both are good skills to practice.

If you want to learn more about using APIs in Python, I recommend starting [here](https://wesmckinney.com/book/accessing-data.html#io_web_apis). 

#### Sources

Kirschbaum, D. B., Adler, R., Hong, Y., Hill, S., & Lerner-Lam, A. (2010). A global landslide catalog for hazard applications: method, results, and limitations. Natural Hazards, 52(3), 561â€“575. doi:10.1007/s11069-009-9401-4.

Kirschbaum, D.B., T. Stanley, Y. Zhou (In press, 2015). Spatial and Temporal Analysis of a Global Landslide Catalog. Geomorphology. doi:10.1016/j.geomorph.2015.03.016.

(These are the citations I mentioned in the beginning ðŸ˜Š)
