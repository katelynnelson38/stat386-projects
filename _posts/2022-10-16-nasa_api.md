---
layout: post
title:  "Access NASA's Landslide Data"
date:   2022-10-16
author: Katelyn Nelson
description: Use Python to access global landslide data with NASAâ€™s API
image: /assets/images/apipost/nasa-vhSz50AaFAs-unsplash.jpg
<!-- image: /assets/images/apipost/dyaa-eldin-moustafa-nXlUIwIaH84-unsplash2.jpg -->
<!--image: /assets/images/apipost/1C1701ED-D459-4F9D-960D-6C10645CB3AE.PNG -->
---

Every data analysis begins with 'data' (see what I did there? ðŸ˜‰). So where can you get data without putting the effort into collecting your own? An application programming interface (API) is a powerful tool that acts as a bridge between your device and a database so that you can access existing data. To use an API you need a unique 'key' that acts as a personal password to authenticate your access to the data. There are a lot of free APIs online that you can request keys for, but sometimes they require a paid subscription.

Even though you can access data, make sure that you are using it according to the source's conditions. I will be using NASA's Open Data Portal which has a ton of pictures, data, and other resources that are available for free as long as you agree to their Terms and Conditions and cite the data.

**Note:** I will not include all of my code in this post, but if you have difficulty following along, the Jupyter Notebook file that I used to complete the steps below can be found in my github repository [nasa_api](https://github.com/katelynnelson38/nasa_api).

# Set up

Before we can start, you will need to [create an account](https://data.nasa.gov/login) to NASA's Open Data Portal and [request an API key](https://api.nasa.gov/index.html#signUp). I recommend copying your API key and pasting it into text file for later use. In this post I will be using NASA's [Global Landslide Catalog Export](https://data.nasa.gov/Earth-Science/Global-Landslide-Catalog-Export/dd9e-wu2v) data.

I used the "BeautifulSoup" and "requests" packages in Python to pull data from the NASA API, so make sure to import those. You'll need additional packages for the data cleaning, but I will not address those in depth.

```
from bs4 import BeautifulSoup
import requests
```

# Get the data

For privacy and legal purposes, I will not share my API key, but if you requested one it should work the same. I pasted mine into a file named `nasa_apikey.txt`. Substitute the name of your file, or the API key.

```
with open('nasa_apikey.txt', 'r') as file:
    nasa_key = file.read()

payload = {}

headers= {
  "apikey": nasa_key
}
```

Now you can use your key (stored as a header) to request data from the NASA database. Most often you specify which data you want by the url you use, which can usually be found in the online API documentation.

```
nasa_url = 'https://data.nasa.gov/resource/dd9e-wu2v.json'
r = requests.get(nasa_url, headers=headers, data = payload)
```

Use `.json()` to read the data and create a pandas dataframe.

```
landslide = r.json()
landslide_df = pd.DataFrame(landslide)
```

# Clean the data

Now that we have the data, we can use the `shape` attribute to see that it includes 1000 observations with 30 variables. Let's narrow it down to 9 variables that we might want to use for an analysis.

```
landslide_df = landslide_df[['event_date', 'landslide_category', 'landslide_trigger', 
'landslide_size', 'fatality_count', 'injury_count', 'country_name', 'longitude', 'latitude']]
```

One powerful tool in exploring new data is the `info()` function. When you use the function on a dataframe (like this: `df.info()`) it will return how many columns the dataframe has, how many non-null values each column has, and the variable type.

```
landslide_df.info()
```

![first info](https://raw.githubusercontent.com/katelynnelson38/stat386-projects/main/assets/images/apipost/first.info.PNG)

This data is has quite a few null values and all the variable types are objects. We will have to fix this before the data is ready for an analysis. 

Start by doing the following:

- The null values in `fatality_count` and `injury_count` correspond with no fatalities or injuries
- When landslide_size, `landslide_trigger`, and `country_name` are null, we will fill it with `unknown`
- Columns `fatality_count`, `injury_count`, `latitude`, and `longitude` need to be type float
- Extract a simpler version of `event_date` (year-month-day) and convert it to type datetime

(Remember, if you need help look at how I did it in my [nasa_api](https://github.com/katelynnelson38/nasa_api) repository.)

Using `info()` again after cleaning, we can see that our data does not have any null values and the columns are the correct variable type (hooray ðŸŽ‰).

![second info](https://raw.githubusercontent.com/katelynnelson38/stat386-projects/main/assets/images/apipost/second.info.PNG)

Your cleaned dataframe should resemble this:

![landslide df](https://raw.githubusercontent.com/katelynnelson38/stat386-projects/main/assets/images/apipost/cleaned_df.PNG)

Now the data is ready to be used for analysis!

# Export the data

Once you have your data, you may want to export it to share with others or for later use.

```
landslide_df.to_csv("landslide", index = False)
```

**Note:** Once you export the data to a csv, the variable types will be lost. If you read it in, all variables will be type object.

# You did it!

What now? Well, you can use the data to learn about common triggers of landslides, the effects of landslides, the frequency of landslides in a certain country, the most common time of year for landslides, etc. There are so many possibilties, so stay tuned ðŸ‘€ for my next post where I will do an exploratory data analysis (EDA) with the landslide dataframe we just created!

As you can see, there is so much data available for your exploration, you just need to know how to access it! APIs are powerful tools that allow you to access lots of cool data. Web scraping can also produce intersting data. I prefer using API's because the data usually turns out a little tidier, but both are good skills to practice.

If you want to learn more about using APIs in Python, I recommend starting [here](https://wesmckinney.com/book/accessing-data.html#io_web_apis). 

### Sources

Kirschbaum, D. B., Adler, R., Hong, Y., Hill, S., & Lerner-Lam, A. (2010). A global landslide catalog for hazard applications: method, results, and limitations. Natural Hazards, 52(3), 561â€“575. doi:10.1007/s11069-009-9401-4.

Kirschbaum, D.B., T. Stanley, Y. Zhou (In press, 2015). Spatial and Temporal Analysis of a Global Landslide Catalog. Geomorphology. doi:10.1016/j.geomorph.2015.03.016.

(These are the citations I mentioned in the beginning ðŸ˜Š)
